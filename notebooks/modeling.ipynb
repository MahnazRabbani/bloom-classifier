{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a Manageable Sample  \n",
    "\n",
    "\n",
    "Starting from this section, we will work with a smaller sample of our dataset. By using this sample, we can run and iterate faster, enabling quicker experimentation and refinement of our algorithms. Once we have fine-tuned our approach and are confident in its effectiveness, we can scale up to the full dataset for the final model training and evaluation. This approach will help us efficiently develop a robust Bloom Classifier while reducing processing time and computational resources during the experimentation phase. Let's make the most of this manageable sample to optimize our workflow and achieve our goals more efficiently.      \n",
    "\n",
    "It's generally a good idea to make sure that our sample includes all classes, especially in a multi-class problem such as this. This is important because the model needs to learn features from all the classes during the training process. If some classes are missing in the training set, our model won't be able to recognize them during inference.\n",
    "\n",
    "If the classes are imbalanced (which is the case here), we might also want to consider using stratified sampling, which ensures that the distribution of classes in your sample matches the distribution in the full dataset. This can help the model learn more effectively, particularly for classes that have fewer examples. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahnaz/vscodeProjects/bloom-classifier/venv_bloom-classifier/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/mahnaz/vscodeProjects/bloom-classifier/venv_bloom-classifier/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 4.6788 Acc: 0.0141\n",
      "val Loss: 4.6518 Acc: 0.0312\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 4.5820 Acc: 0.0328\n",
      "val Loss: 4.5787 Acc: 0.0437\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 4.4959 Acc: 0.0312\n",
      "val Loss: 4.5572 Acc: 0.0250\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 4.4374 Acc: 0.0406\n",
      "val Loss: 4.5484 Acc: 0.0250\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 4.3977 Acc: 0.0484\n",
      "val Loss: 4.5429 Acc: 0.0187\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Specify the size of the sample you want to create\n",
    "sample_size = 800  # Adjust this as needed\n",
    "\n",
    "# Create a StratifiedShuffleSplit instance\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=sample_size, random_state=0)\n",
    "\n",
    "# Load the data\n",
    "image_folder = './data/jpg/'\n",
    "image_files = [os.path.join(image_folder, file) for file in os.listdir(image_folder)]\n",
    "labels_file = \"./data/imagelabels.mat\"\n",
    "labels_data = loadmat(labels_file)\n",
    "labels = labels_data['labels'][0] - 1  # Adjusting labels to be in the range 0-101\n",
    "\n",
    "# Get index for the sample\n",
    "for train_index, sample_index in sss.split(image_files, labels):\n",
    "    image_files_sample = [image_files[i] for i in sample_index]\n",
    "    labels_sample = [labels[i] for i in sample_index]\n",
    "\n",
    "# Now use this sample to split into training and validation data\n",
    "image_files_train, image_files_val, labels_train, labels_val = train_test_split(image_files_sample, labels_sample, test_size=0.2, stratify=labels_sample)\n",
    "\n",
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_files[idx])  # Open image file\n",
    "        image = image.convert('RGB')  # Convert image to RGB channels\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transforms for the training data and testing data\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "# Create the custom dataset for training and validation data\n",
    "dataset_train = FlowerDataset(image_files_train, labels_train, transform=train_transforms)\n",
    "dataset_val = FlowerDataset(image_files_val, labels_val, transform=test_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=64, shuffle=False)\n",
    "\n",
    "dataloaders = {'train': dataloader_train, 'val': dataloader_val}\n",
    "\n",
    "# Specify the device for computation\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "dataset_sizes = {'train': len(dataset_train), 'val': len(dataset_val)}\n",
    "\n",
    "\n",
    "# Get a pretrained ResNet model and modify the final layer\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model_flowers.pth')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# data directories\n",
    "\n",
    "image_folder = './data/jpg/'\n",
    "image_files = [os.path.join(image_folder, file) for file in os.listdir(image_folder)]\n",
    "\n",
    "labels_file = \"./data/imagelabels.mat\"\n",
    "labels_data = scipy.io.loadmat(labels_file)\n",
    "labels = labels_data['labels'][0] - 1  # Adjusting labels to be in the range 0-101\n",
    "\n",
    "# Create the custom dataset\n",
    "\n",
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_files[idx])  # Open image file\n",
    "        image = image.convert('RGB')  # Convert image to RGB channels\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models, transforms\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the size of the sample you want to create\n",
    "sample_size = 500  # Adjust this as needed\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset instance and apply the transform\n",
    "dataset = FlowerDataset(image_files, labels, transform=transform)\n",
    "\n",
    "# Split the dataset into training, validation and test sets\n",
    "n_train = int(len(dataset) * 0.7)\n",
    "n_valid = int(len(dataset) * 0.15)\n",
    "n_test = len(dataset) - n_train - n_valid\n",
    "dataset_train, dataset_valid, dataset_test = random_split(dataset, [n_train, n_valid, n_test])\n",
    "\n",
    "# Create data loaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=64, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=False)\n",
    "\n",
    "# Specify number of classes\n",
    "num_classes = 102\n",
    "\n",
    "# Load pre-trained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Update the classifier part of the pre-trained model\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            if phase == 'train':\n",
    "                dataloader = dataloader_train\n",
    "            else:\n",
    "                dataloader = dataloader_valid\n",
    "\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model_flowers.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_bloom-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acdc56c228ce9299a1ca8585019160b9306fcacb61c64dd72dac66a4be909377"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
