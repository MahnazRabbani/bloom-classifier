{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 18:49:04.802760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-14 18:49:05.521558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "from transformers import DefaultDataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "training_data = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split = \"train\",\n",
    "    download=True,\n",
    "    #transform=ToTensor()\n",
    ")\n",
    "\n",
    "val_data = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split = \"val\",\n",
    "    download=True,\n",
    "    #transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.Flowers102(\n",
    "    root=\"data\",\n",
    "    split = \"test\",\n",
    "    download=True,\n",
    "    #transform=ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "class Flowers102Transformed(Dataset):\n",
    "    def __init__(self, original_dataset, transforms):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.original_dataset[idx]\n",
    "        pixel_values = self.transforms(img.convert(\"RGB\"))\n",
    "        return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "\n",
    "\n",
    "# Define the checkpoint\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "# Define the transformations\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
    "\n",
    "\n",
    "train_data = Flowers102Transformed(training_data, _transforms)\n",
    "val_data = Flowers102Transformed(val_data, _transforms)\n",
    "test_data = Flowers102Transformed(test_data, _transforms)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "from transformers import DefaultDataCollator\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data_size: 1020\n",
      "val_data_size: 1020\n",
      "test_data_size: 6149\n",
      "training_data_size: 6149\n",
      "val_data_size: 1020\n",
      "test_data_size: 1020\n"
     ]
    }
   ],
   "source": [
    "print(\"training_data_size:\", len(training_data))\n",
    "print(\"val_data_size:\", len(val_data))\n",
    "print(\"test_data_size:\", len(test_data))\n",
    "\n",
    "training_data, test_data = test_data, training_data\n",
    "\n",
    "print(\"training_data_size:\", len(training_data))\n",
    "print(\"val_data_size:\", len(val_data))\n",
    "print(\"test_data_size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mahnaz/mlprojects/bloom-classifier/venv_bloom-classifier/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c71f2934b0b459a89aa3ac8f6ea7936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6284, 'learning_rate': 2.5e-05, 'epoch': 0.31}\n",
      "{'loss': 4.5942, 'learning_rate': 5e-05, 'epoch': 0.62}\n",
      "{'loss': 4.5291, 'learning_rate': 4.709302325581396e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff5011e9e254dc5ab2d57edfdc4c71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.447311878204346, 'eval_accuracy': 0.27450980392156865, 'eval_runtime': 21.4513, 'eval_samples_per_second': 47.55, 'eval_steps_per_second': 5.967, 'epoch': 1.0}\n",
      "{'loss': 4.4148, 'learning_rate': 4.418604651162791e-05, 'epoch': 1.25}\n",
      "{'loss': 4.3267, 'learning_rate': 4.127906976744187e-05, 'epoch': 1.56}\n",
      "{'loss': 4.2447, 'learning_rate': 3.837209302325582e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454859668d0a4cac90e2fe1a22dd40f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.18092679977417, 'eval_accuracy': 0.7598039215686274, 'eval_runtime': 21.9573, 'eval_samples_per_second': 46.454, 'eval_steps_per_second': 5.829, 'epoch': 2.0}\n",
      "{'loss': 4.1564, 'learning_rate': 3.5465116279069774e-05, 'epoch': 2.19}\n",
      "{'loss': 4.0352, 'learning_rate': 3.2558139534883724e-05, 'epoch': 2.5}\n",
      "{'loss': 3.9991, 'learning_rate': 2.9651162790697678e-05, 'epoch': 2.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82e1ce4aeb646bba05a9e4e03f30ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.975513219833374, 'eval_accuracy': 0.884313725490196, 'eval_runtime': 22.2832, 'eval_samples_per_second': 45.774, 'eval_steps_per_second': 5.744, 'epoch': 3.0}\n",
      "{'loss': 3.9439, 'learning_rate': 2.674418604651163e-05, 'epoch': 3.12}\n",
      "{'loss': 3.8316, 'learning_rate': 2.3837209302325582e-05, 'epoch': 3.44}\n",
      "{'loss': 3.8242, 'learning_rate': 2.0930232558139536e-05, 'epoch': 3.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b550dc68a9a84f1883e9f466172f27ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.8248324394226074, 'eval_accuracy': 0.9303921568627451, 'eval_runtime': 21.9121, 'eval_samples_per_second': 46.55, 'eval_steps_per_second': 5.842, 'epoch': 4.0}\n",
      "{'loss': 3.7498, 'learning_rate': 1.802325581395349e-05, 'epoch': 4.06}\n",
      "{'loss': 3.702, 'learning_rate': 1.5116279069767441e-05, 'epoch': 4.38}\n",
      "{'loss': 3.6716, 'learning_rate': 1.2209302325581395e-05, 'epoch': 4.69}\n",
      "{'loss': 3.6785, 'learning_rate': 9.302325581395349e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bc0d7ec5a848c596f07382304f797a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7415974140167236, 'eval_accuracy': 0.9323529411764706, 'eval_runtime': 22.0515, 'eval_samples_per_second': 46.255, 'eval_steps_per_second': 5.805, 'epoch': 5.0}\n",
      "{'loss': 3.613, 'learning_rate': 6.395348837209303e-06, 'epoch': 5.31}\n",
      "{'loss': 3.6073, 'learning_rate': 3.488372093023256e-06, 'epoch': 5.62}\n",
      "{'loss': 3.5874, 'learning_rate': 5.813953488372093e-07, 'epoch': 5.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eed662c1d404a0b9d5ca17fce1899c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7175724506378174, 'eval_accuracy': 0.9303921568627451, 'eval_runtime': 21.8259, 'eval_samples_per_second': 46.733, 'eval_steps_per_second': 5.865, 'epoch': 6.0}\n",
      "{'train_runtime': 468.7241, 'train_samples_per_second': 13.057, 'train_steps_per_second': 0.41, 'train_loss': 4.003167939682801, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=192, training_loss=4.003167939682801, metrics={'train_runtime': 468.7241, 'train_samples_per_second': 13.057, 'train_steps_per_second': 0.41, 'train_loss': 4.003167939682801, 'epoch': 6.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels= 102 #len(labels),\n",
    "    #id2label=id2label,\n",
    "    #label2id=label2id,\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"vit_fulldataset\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8, # memory error with 16\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=6,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_bloom-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acdc56c228ce9299a1ca8585019160b9306fcacb61c64dd72dac66a4be909377"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
